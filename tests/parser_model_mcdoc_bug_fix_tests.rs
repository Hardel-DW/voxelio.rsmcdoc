//! Tests unitaires spÃ©cifiques pour le bug model.mcdoc
//! Applique la mÃ©thodologie de debugging stricte avec 3 hypothÃ¨ses distinctes

use voxel_rsmcdoc::lexer::Lexer;
use voxel_rsmcdoc::parser::Parser;

/// HYPOTHÃˆSE 1: Arrays avec contraintes imbriquÃ©es [float @ -80..80] @ 3
#[test]
fn test_hypothesis_1_nested_array_constraints() {
    // Test seulement la partie type problÃ©matique
    let input = "[float @ -80..80] @ 3";
    
    println!("ğŸ§ª TEST HYPOTHÃˆSE 1 - Input: {}", input);
    
    // 1. TOKENISATION
    let mut lexer = Lexer::new(input);
    let tokens_result = lexer.tokenize();
    
    assert!(tokens_result.is_ok(), "âŒ Tokenisation failed: {:?}", tokens_result.err());
    let tokens = tokens_result.unwrap();
    
    println!("ğŸ” Tokens gÃ©nÃ©rÃ©s:");
    for (i, token) in tokens.iter().enumerate() {
        println!("  {}: {:?}", i, token);
    }
    
    // 2. PARSING DU TYPE
    let mut parser = Parser::new(tokens);
    let type_result = parser.parse_type_expression();
    
    println!("ğŸ” RÃ©sultat parsing type: {:?}", type_result);
    
    if let Err(ref error) = type_result {
        println!("âŒ Erreur de parsing type:");
        println!("  - {:?}", error);
    }
    
    // Ce test doit rÃ©ussir une fois corrigÃ©
    assert!(type_result.is_ok(), "âŒ Parser failed on nested array constraints type");
    
    // VÃ©rifier la structure AST attendue
    if let Ok(ast) = type_result {
        match ast {
            voxel_rsmcdoc::parser::TypeExpression::Array { element_type, constraints } => {
                println!("âœ… Array type parsÃ© correctement");
                println!("  - Element type: {:?}", element_type);
                println!("  - Constraints: {:?}", constraints);
                
                // VÃ©rifier que les contraintes externes sont correctes (@ 3)
                if let Some(constraints) = constraints {
                    assert_eq!(constraints.min, Some(3));
                    assert_eq!(constraints.max, Some(3));
                } else {
                    panic!("âŒ Expected array constraints");
                }
            }
            _ => panic!("âŒ Expected Array type, got {:?}", ast),
        }
    }
}

/// HYPOTHÃˆSE 2: Nombres nÃ©gatifs dans les contraintes
#[test]
fn test_hypothesis_2_negative_numbers_in_constraints() {
    // Test seulement la partie contrainte avec nombres nÃ©gatifs
    let input = "int @ -80..80";
    
    println!("ğŸ§ª TEST HYPOTHÃˆSE 2 - Input: {}", input);
    
    let mut lexer = Lexer::new(input);
    let tokens_result = lexer.tokenize();
    
    assert!(tokens_result.is_ok(), "âŒ Tokenisation failed: {:?}", tokens_result.err());
    let tokens = tokens_result.unwrap();
    
    println!("ğŸ” Tokens pour nombres nÃ©gatifs:");
    for (i, token) in tokens.iter().enumerate() {
        println!("  {}: {:?}", i, token);
    }
    
    // VÃ©rifier que -80 est tokenisÃ© comme Number(-80.0) et pas Minus + Number(80.0)
    let negative_number_found = tokens.iter().any(|t| {
        if let voxel_rsmcdoc::lexer::Token::Number(n) = &t.token {
            *n == -80.0
        } else {
            false
        }
    });
    
    assert!(negative_number_found, "âŒ Nombre nÃ©gatif -80 pas trouvÃ© dans les tokens");
    
    let mut parser = Parser::new(tokens);
    let type_result = parser.parse_type_expression();
    
    println!("ğŸ” RÃ©sultat parsing type avec nombres nÃ©gatifs: {:?}", type_result);
    
    // Ce test doit rÃ©ussir - vÃ©rifie que les contraintes avec nombres nÃ©gatifs parsent correctement
    assert!(type_result.is_ok(), "âŒ Parser failed on negative number constraints");
}

/// HYPOTHÃˆSE 3: Structure complÃ¨te du champ model.mcdoc
#[test]
fn test_hypothesis_3_complete_model_field_structure() {
    // Structure complÃ¨te problÃ©matique
    let input = r#"
    struct ModelDisplay {
        [CustomizableItemDisplayContext]: struct ItemTransform {
            rotation?: [float] @ 3,
            translation?: [float @ -80..80] @ 3,
            scale?: [float @ -4..4] @ 3,
        },
    }
    "#;
    
    println!("ğŸ§ª TEST HYPOTHÃˆSE 3 - Structure complÃ¨te");
    
    let mut lexer = Lexer::new(input);
    let tokens_result = lexer.tokenize();
    
    assert!(tokens_result.is_ok(), "âŒ Tokenisation failed: {:?}", tokens_result.err());
    let tokens = tokens_result.unwrap();
    
    println!("ğŸ” Nombre total de tokens: {}", tokens.len());
    
    let mut parser = Parser::new(tokens);
    let parse_result = parser.parse();
    
    println!("ğŸ” RÃ©sultat parsing structure complÃ¨te: {:?}", parse_result);
    
    if let Err(ref errors) = parse_result {
        println!("âŒ Erreurs de parsing:");
        for (i, error) in errors.iter().enumerate() {
            println!("  {}: {:?}", i, error);
        }
    }
    
    // Ce test doit rÃ©ussir une fois corrigÃ©
    assert!(parse_result.is_ok(), "âŒ Parser failed on complete model structure");
}

/// TEST DE TRAÃ‡AGE: Tracer ligne par ligne le parsing problÃ©matique
#[test]
fn test_trace_parsing_step_by_step() {
    // Cas minimal reproduisant le bug
    let input = "[float @ -80..80] @ 3";
    
    println!("ğŸ” TRAÃ‡AGE DÃ‰TAILLÃ‰ - Input: {}", input);
    
    // Ã‰TAPE 1: Tokenisation
    let mut lexer = Lexer::new(input);
    let tokens_result = lexer.tokenize();
    assert!(tokens_result.is_ok(), "Tokenisation failed");
    let tokens = tokens_result.unwrap();
    
    println!("ğŸ” Ã‰TAPE 1 - Tokens:");
    for (i, token) in tokens.iter().enumerate() {
        println!("  Token[{}]: {:?} at pos {:?}", i, token.token, token.position);
    }
    
    // Ã‰TAPE 2: Parsing avec traÃ§age
    let mut parser = Parser::new(tokens);
    
    // Tenter de parser comme type expression
    let type_result = parser.parse_type_expression();
    
    println!("ğŸ” Ã‰TAPE 2 - RÃ©sultat parse_type_expression: {:?}", type_result);
    
    // Analyser oÃ¹ exactement Ã§a Ã©choue
    if let Err(ref error) = type_result {
        println!("âŒ ERREUR DÃ‰TAILLÃ‰E:");
        println!("  Message: {:?}", error);
        if let Some(pos) = error.position() {
            println!("  Position: Ligne {}, Colonne {}", pos.line, pos.column);
        } else {
            println!("  Position: Non disponible");
        }
    }
}

/// TEST DE VALIDATION: Exemple d'array 2D correct
#[test]
fn test_array_2d_syntax_validation() {
    // Test que la syntaxe d'array 2D fonctionne correctement
    let simple_case = "[string] @ 3";
    let complex_case = "[float @ -4..4] @ 3";
    
    for (name, input) in [("simple", simple_case), ("complex", complex_case)] {
        println!("ğŸ§ª TEST ARRAY 2D {} - Input: {}", name, input);
        
        let mut lexer = Lexer::new(input);
        let tokens = lexer.tokenize().expect("Tokenisation should work");
        
        let mut parser = Parser::new(tokens);
        let result = parser.parse_type_expression();
        
        println!("ğŸ” RÃ©sultat {}: {:?}", name, result);
        
        match result {
            Ok(ast) => println!("âœ… {} parsed successfully: {:?}", name, ast),
            Err(error) => {
                println!("âŒ {} failed: {:?}", name, error);
                assert!(false, "Array 2D syntax should parse correctly for {}", name);
            }
        }
    }
} 